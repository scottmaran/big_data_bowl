{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c17a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2378d714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>gameId</th>\n",
       "      <th>playId</th>\n",
       "      <th>playIndex</th>\n",
       "      <th>timeIndex</th>\n",
       "      <th>time</th>\n",
       "      <th>playerId</th>\n",
       "      <th>football_pos</th>\n",
       "      <th>state</th>\n",
       "      <th>next_state</th>\n",
       "      <th>reward</th>\n",
       "      <th>action</th>\n",
       "      <th>next_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>2018090600</td>\n",
       "      <td>677</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-09-07T01:34:34.500</td>\n",
       "      <td>44979.0</td>\n",
       "      <td>[[11.15, 42.28, 0.17, 2.91, 6.78, 0.0, 0.0, 11...</td>\n",
       "      <td>[[11.53, 42.45, 0.14, 1.32, 3.03, 117.3, 148.0...</td>\n",
       "      <td>[[11.62, 42.32, 0.16, 1.7, 3.13, 114.42, 144.8...</td>\n",
       "      <td>[-0.09]</td>\n",
       "      <td>[[-0.09, 0.13, -0.02, -0.38, -0.1, 2.88, 3.22,...</td>\n",
       "      <td>[[-0.16, 0.16, -0.07, -0.53, -0.24, -17.77, 7....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>2018090600</td>\n",
       "      <td>677</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-09-07T01:34:34.600</td>\n",
       "      <td>44979.0</td>\n",
       "      <td>[[11.51, 42.1, 0.39, 1.64, 4.44, 0.0, 0.0, 11....</td>\n",
       "      <td>[[11.62, 42.32, 0.16, 1.7, 3.13, 114.42, 144.8...</td>\n",
       "      <td>[[11.78, 42.16, 0.23, 2.23, 3.37, 132.19, 137....</td>\n",
       "      <td>[-0.16]</td>\n",
       "      <td>[[-0.16, 0.16, -0.07, -0.53, -0.24, -17.77, 7....</td>\n",
       "      <td>[[-0.17, 0.18, -0.01, -0.4, -0.51, 6.53, 2.66,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>2018090600</td>\n",
       "      <td>677</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-09-07T01:34:34.700</td>\n",
       "      <td>44979.0</td>\n",
       "      <td>[[11.47, 42.06, 0.06, 1.2, 3.4, 0.0, 0.0, 11.4...</td>\n",
       "      <td>[[11.78, 42.16, 0.23, 2.23, 3.37, 132.19, 137....</td>\n",
       "      <td>[[11.95, 41.98, 0.24, 2.63, 3.88, 125.66, 134....</td>\n",
       "      <td>[-0.17]</td>\n",
       "      <td>[[-0.17, 0.18, -0.01, -0.4, -0.51, 6.53, 2.66,...</td>\n",
       "      <td>[[-0.21, 0.19, -0.05, -0.48, -0.22, 4.32, 3.47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>2018090600</td>\n",
       "      <td>677</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-09-07T01:34:34.800</td>\n",
       "      <td>44979.0</td>\n",
       "      <td>[[12.39, 41.68, 1.0, 0.93, 2.17, 0.0, 0.0, 12....</td>\n",
       "      <td>[[11.95, 41.98, 0.24, 2.63, 3.88, 125.66, 134....</td>\n",
       "      <td>[[12.16, 41.79, 0.29, 3.11, 4.1, 121.34, 131.2...</td>\n",
       "      <td>[-0.21]</td>\n",
       "      <td>[[-0.21, 0.19, -0.05, -0.48, -0.22, 4.32, 3.47...</td>\n",
       "      <td>[[-0.25, 0.21, -0.03, -0.44, -0.67, 2.49, 3.83...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>2018090600</td>\n",
       "      <td>677</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-07T01:34:34.900</td>\n",
       "      <td>44979.0</td>\n",
       "      <td>[[13.5, 41.18, 1.22, 1.47, 3.5, 0.0, 0.0, 13.5...</td>\n",
       "      <td>[[12.16, 41.79, 0.29, 3.11, 4.1, 121.34, 131.2...</td>\n",
       "      <td>[[12.41, 41.58, 0.32, 3.55, 4.77, 118.85, 127....</td>\n",
       "      <td>[-0.25]</td>\n",
       "      <td>[[-0.25, 0.21, -0.03, -0.44, -0.67, 2.49, 3.83...</td>\n",
       "      <td>[[-0.32, 0.21, -0.06, -0.47, -0.03, 0.0, 9.25,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  season      gameId  playId  playIndex  timeIndex                     time  \\\n",
       "0   2018  2018090600     677          0          0  2018-09-07T01:34:34.500   \n",
       "1   2018  2018090600     677          0          1  2018-09-07T01:34:34.600   \n",
       "2   2018  2018090600     677          0          2  2018-09-07T01:34:34.700   \n",
       "3   2018  2018090600     677          0          3  2018-09-07T01:34:34.800   \n",
       "4   2018  2018090600     677          0          4  2018-09-07T01:34:34.900   \n",
       "\n",
       "   playerId                                       football_pos  \\\n",
       "0   44979.0  [[11.15, 42.28, 0.17, 2.91, 6.78, 0.0, 0.0, 11...   \n",
       "1   44979.0  [[11.51, 42.1, 0.39, 1.64, 4.44, 0.0, 0.0, 11....   \n",
       "2   44979.0  [[11.47, 42.06, 0.06, 1.2, 3.4, 0.0, 0.0, 11.4...   \n",
       "3   44979.0  [[12.39, 41.68, 1.0, 0.93, 2.17, 0.0, 0.0, 12....   \n",
       "4   44979.0  [[13.5, 41.18, 1.22, 1.47, 3.5, 0.0, 0.0, 13.5...   \n",
       "\n",
       "                                               state  \\\n",
       "0  [[11.53, 42.45, 0.14, 1.32, 3.03, 117.3, 148.0...   \n",
       "1  [[11.62, 42.32, 0.16, 1.7, 3.13, 114.42, 144.8...   \n",
       "2  [[11.78, 42.16, 0.23, 2.23, 3.37, 132.19, 137....   \n",
       "3  [[11.95, 41.98, 0.24, 2.63, 3.88, 125.66, 134....   \n",
       "4  [[12.16, 41.79, 0.29, 3.11, 4.1, 121.34, 131.2...   \n",
       "\n",
       "                                          next_state   reward  \\\n",
       "0  [[11.62, 42.32, 0.16, 1.7, 3.13, 114.42, 144.8...  [-0.09]   \n",
       "1  [[11.78, 42.16, 0.23, 2.23, 3.37, 132.19, 137....  [-0.16]   \n",
       "2  [[11.95, 41.98, 0.24, 2.63, 3.88, 125.66, 134....  [-0.17]   \n",
       "3  [[12.16, 41.79, 0.29, 3.11, 4.1, 121.34, 131.2...  [-0.21]   \n",
       "4  [[12.41, 41.58, 0.32, 3.55, 4.77, 118.85, 127....  [-0.25]   \n",
       "\n",
       "                                              action  \\\n",
       "0  [[-0.09, 0.13, -0.02, -0.38, -0.1, 2.88, 3.22,...   \n",
       "1  [[-0.16, 0.16, -0.07, -0.53, -0.24, -17.77, 7....   \n",
       "2  [[-0.17, 0.18, -0.01, -0.4, -0.51, 6.53, 2.66,...   \n",
       "3  [[-0.21, 0.19, -0.05, -0.48, -0.22, 4.32, 3.47...   \n",
       "4  [[-0.25, 0.21, -0.03, -0.44, -0.67, 2.49, 3.83...   \n",
       "\n",
       "                                         next_action  \n",
       "0  [[-0.16, 0.16, -0.07, -0.53, -0.24, -17.77, 7....  \n",
       "1  [[-0.17, 0.18, -0.01, -0.4, -0.51, 6.53, 2.66,...  \n",
       "2  [[-0.21, 0.19, -0.05, -0.48, -0.22, 4.32, 3.47...  \n",
       "3  [[-0.25, 0.21, -0.03, -0.44, -0.67, 2.49, 3.83...  \n",
       "4  [[-0.32, 0.21, -0.06, -0.47, -0.03, 0.0, 9.25,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Read in saved dataset to use to feed into the model.\n",
    "Further pre-processing still needs to be done on the state vectors\n",
    "'''\n",
    "\n",
    "data_df = pd.read_pickle('/home/amans/Development/scott/nfl-big-data-bowl-2022/clean_dir/ball_carrier_data.pkl')\n",
    "\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b397880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.15300000e+01,  4.24500000e+01,  1.40000000e-01,  1.32000000e+00,\n",
       "        3.03000000e+00,  1.17300000e+02,  1.48060000e+02,  1.15300000e+01,\n",
       "        1.58000000e+01,  2.73000000e+01,  5.80600000e+01,  8.27327901e-01,\n",
       "       -5.61719276e-01,  9.98232532e-01,  5.94290537e-02,  0.00000000e+00,\n",
       "        1.02310000e+01,  3.04200000e+01])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "State vector \n",
    "['x','y', 'dis', 's', 'a', 'o', 'dir', 'adj_x', 'adj_y', 'adj_o', 'adj_dir', 'sin_adj_o', 'cos_adj_o', \\\n",
    "'sin_adj_dir', 'cos_adj_dir', 'dist_from_ball_carrier', 'min_teammate_dist', 'min_opponent_dist']\n",
    "'''\n",
    "\n",
    "data_df.loc[0,'state'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9526514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "723edf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a40f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size =  torch.Size([192049])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create torch dataset\n",
    "'''\n",
    "\n",
    "features = torch.zeros(len(data_df)).to(device)\n",
    "labels = torch.zeros(len(data_df)).to(device)\n",
    "\n",
    "print(\"size = \", features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91376685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['season', 'gameId', 'playId', 'playIndex', 'timeIndex', 'time',\n",
      "       'playerId', 'football_pos', 'state', 'next_state', 'reward', 'action',\n",
      "       'next_action'],\n",
      "      dtype='object')\n",
      "Length of training data: 134434\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Function that splits the data into a training, validation, and test set\n",
    "'''\n",
    "def split_data(dataset, train_split, seed):\n",
    "    np.random.seed(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_num = int(len(dataset)*train_split)\n",
    "    val_num = (len(dataset) - int(len(dataset)*train_split))//2\n",
    "\n",
    "    train_indices = indices[0:train_num]\n",
    "    val_indices = indices[train_num:train_num+val_num]\n",
    "    test_indices = indices[train_num+val_num:]\n",
    "\n",
    "    #check to make sure slices correct\n",
    "    assert len(dataset) == len(train_indices) + len(val_indices) + len(test_indices)\n",
    "\n",
    "    #dataset = help.normalize(train_indices, dataset)\n",
    "\n",
    "    train_data = dataset.iloc[train_indices,:]\n",
    "    val_data = dataset.iloc[val_indices,:]\n",
    "    test_data = dataset.iloc[test_indices,:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "train_data, val_data, test_data = split_data(data_df, 0.7, 2430)\n",
    "\n",
    "print(test_data.columns)\n",
    "\n",
    "print(f\"Length of training data: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "200b93b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 18\n",
      "Action size: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFirst CNN model\\ncnn_model = nn.Sequential(\\n    nn.Conv2d(1, 12, (12,1), stride=1, padding=0),\\n    nn.LeakyReLU(),\\n    nn.Conv2d(12, 24, (6, 1)),\\n    nn.LeakyReLU(),\\n    nn.Conv2d(24, 48, (1,1)),\\n    nn.Flatten(),\\n    nn.Linear(48*6*6,action_size)\\n)\\n\\ncnn_model.to(device=device)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Predict ['s', 'a', 'adj_x', 'adj_y', 'adj_o', 'adj_dir', 'sin_adj_o', 'cos_adj_o', 'sin_adj_dir', 'cos_adj_dir']\n",
    "from \n",
    "['x','y', 'dis', 's', 'a', 'o', 'dir', 'adj_x', 'adj_y', 'adj_o', 'adj_dir', 'sin_adj_o', 'cos_adj_o', \\\n",
    "'sin_adj_dir', 'cos_adj_dir', 'dist_from_ball_carrier', 'min_teammate_dist', 'min_opponent_dist']\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Define hyperparamters\n",
    "'''\n",
    "BATCH_SIZE = 32\n",
    "state_size = len(data_df.loc[0,'state'][0])\n",
    "action_size = 8\n",
    "gamma = 0.99\n",
    "#max_action = torch.tensor(max_action, dtype=torch.float32).to(device=device)\n",
    "\n",
    "print(f\"State size: {state_size}\")\n",
    "print(f\"Action size: {action_size}\")\n",
    "    \n",
    "'''\n",
    "First CNN model\n",
    "cnn_model = nn.Sequential(\n",
    "    nn.Conv2d(1, 12, (12,1), stride=1, padding=0),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(12, 24, (6, 1)),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(24, 48, (1,1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(48*6*6,action_size)\n",
    ")\n",
    "\n",
    "cnn_model.to(device=device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c950c557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e408d950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTry model predicting entire sequence\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Try model predicting entire sequence\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "97ca561e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=185, out_features=230, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): Linear(in_features=230, out_features=184, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "same results, not great val loss and then overfit\n",
    "'''\n",
    "# cnn_model = nn.Sequential(\n",
    "#     nn.Conv2d(1, 32, (3,3), stride=1,padding=0),\n",
    "#     nn.LeakyReLU(),\n",
    "#     #nn.MaxPool2d((2,2), stride=2),\n",
    "#     nn.Conv2d(32, 64, (3,3), stride=1,padding=0),\n",
    "#     nn.LeakyReLU(),\n",
    "#     #nn.MaxPool2d((2,2), stride=2),\n",
    "#     nn.Conv2d(64,128, (2,2), stride=1,padding=0),\n",
    "#     nn.LeakyReLU(),\n",
    "#     #nn.MaxPool2d((2,2), stride=2),\n",
    "#     nn.Conv2d(128,256, (1,1), stride=1,padding=0),\n",
    "#     nn.LeakyReLU(),\n",
    "#     #nn.MaxPool2d((2,2), stride=2),\n",
    "#     #nn.Conv2d(256,512, (3,3), stride=1,padding=0),\n",
    "#     #nn.LeakyReLU(),\n",
    "#     #nn.MaxPool2d((2,2), stride=2),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(13824, 6000),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Linear(6000, 2000),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Linear(2000, 500),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Linear(500, action_size*23)\n",
    "# )\n",
    "\n",
    "'''\n",
    "val loss @ .3 after 4 epochs, .287 after 7 epochs, .286 after 8 epochs\n",
    "'''\n",
    "# cnn_model = nn.Sequential(\n",
    "#     nn.Conv2d(1, 6, (2,2), stride=1, padding=0),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(924,400), \n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Linear(400,action_size*23)\n",
    "# )\n",
    "\n",
    "'''\n",
    "Similar val losses. .359 val loss after 3 epochs, .313 after 4\n",
    "'''\n",
    "# cnn_model = nn.Sequential(\n",
    "#     nn.Conv2d(1, 3, (2,2), stride=1, padding=0),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(462,230), \n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Linear(230,action_size*23)\n",
    "# )\n",
    "\n",
    "'''\n",
    "Similar val losses, .283 val loss after 2 epochs, .281 after 4\n",
    "'''\n",
    "# cnn_model = nn.Sequential(\n",
    "#     nn.BatchNorm2d(1),\n",
    "#     nn.Conv2d(1, 32, (8,8), stride=1, padding=0),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(128*4,230), \n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Linear(230,action_size*23)\n",
    "# )\n",
    "\n",
    "\n",
    "'''\n",
    "Predicting next state\n",
    "'''\n",
    "'''\n",
    "class cnnModel(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(cnnModel, self).__init__()\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(8)\n",
    "        self.conv1 = nn.Conv2d(1, 32, (8,8), stride=1, padding=0)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(128*4,230)\n",
    "        self.leaky = nn.LeakyReLU()\n",
    "        self.linear2 = nn.Linear(230,action_size*23)\n",
    "        \n",
    "        self.drop = nn.Dropout(.5)\n",
    "        \n",
    "    # input (batch_size, 1, 23, action_size)\n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "        #tens_reshape = input_tensor.view(-1, 23, 8)\n",
    "        initial_shape = input_tensor.shape\n",
    "        \n",
    "        batched_input = self.batch_norm(torch.transpose(input_tensor.view(-1, 23, 8), 1,2))\n",
    "        \n",
    "        inp = torch.transpose(batched_input, 1, 2).view(initial_shape)\n",
    "        \n",
    "        output = self.conv1(inp)\n",
    "        output = self.flatten(output)\n",
    "        output = self.drop(output)\n",
    "        output = self.linear1(output)\n",
    "        output = self.leaky(output)\n",
    "        output = self.linear2(output)\n",
    "        \n",
    "        return output\n",
    "cnn_model = cnnModel(action_size)\n",
    "'''\n",
    "\n",
    "# after 50 epochs, 0.3251 val loss with small dataset model\n",
    "\n",
    "cnn_model = nn.Sequential(\n",
    "    nn.Linear(185,230), \n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(230,184)\n",
    ")\n",
    "\n",
    "cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "90e4f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "#cnn_model.load_state_dict(torch.load(\"full_dataset_model.pt\"))\n",
    "\n",
    "# define loss function\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "#mse_loss_fn = nn.L1Loss()\n",
    "# define optimizers\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.005)\n",
    "\n",
    "#scheduler = ExponentialLR(cnn_optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc2de624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42412.]\n",
      " [48988.]\n",
      " [46309.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_play = train_data.iloc[[2,3,4],:]\n",
    "\n",
    "ids = np.array(test_play.loc[:,'playerId']).reshape(3,1)\n",
    "\n",
    "print(ids)\n",
    "\n",
    "np.append(np.zeros((3,2)), ids, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5cce47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given indices, train model on batch\n",
    "['x','y', 'dis', 's', 'a', 'o', 'dir', 'adj_x', 'adj_y', 'adj_o', 'adj_dir', 'sin_adj_o', 'cos_adj_o', \\\n",
    "'sin_adj_dir', 'cos_adj_dir', 'dist_from_ball_carrier', 'min_teammate_dist', 'min_opponent_dist']\n",
    "\n",
    "Try adding player id as feature\n",
    "'''\n",
    "def train_loop_large(df, indices, model, loss_fn, optimizer, device):\n",
    "    size = len(indices)\n",
    "    play = df.iloc[indices, :]\n",
    "    \n",
    "    numpy_state = np.stack(play['state'].values)\n",
    "    batch_size = numpy_state.shape[0]\n",
    "    playerId = np.array(play.loc[:,'playerId']).reshape(batch_size, 1)\n",
    "\n",
    "    numpy_reshaped_state = np.stack(play['state'].values)[:,:,[3,4,7,8,11,12,13,14]].reshape(batch_size, -1)\n",
    "    numpy_with_playerid = np.append(numpy_reshaped_state, playerId, axis=1)\n",
    "    true_state = torch.tensor(numpy_with_playerid, dtype=torch.float32).to(device=device)\n",
    "    \n",
    "    numpy_reshaped_next_state = np.stack(play['next_state'].values)[:,:,[3,4,7,8,11,12,13,14]].reshape(batch_size, -1)\n",
    "    #numpy_next_with_playerid = np.append(numpy_reshaped_next_state, playerId, axis=1)\n",
    "    true_next_state = torch.tensor(numpy_reshaped_next_state, dtype=torch.float32).to(device=device)\n",
    "    \n",
    "    # shape true state into matrix to pass into CNN\n",
    "    #true_state_mod = torch.tensor(numpy_state_input, dtype=torch.float32).to(device=device)\n",
    "\n",
    "    model.train()  # put model to training mode\n",
    "\n",
    "    #compute prediction and loss\n",
    "    predicted_next_state = model(true_state)\n",
    "    \n",
    "    loss = loss_fn(predicted_next_state, true_next_state)\n",
    "\n",
    "    #Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "80c3d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop_large(test_df, model, loss_fn, device):\n",
    "    size = len(test_df)\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for row_index in range(0,len(test_df)):\n",
    "\n",
    "            play = test_df.iloc[row_index, :]\n",
    "\n",
    "            numpy_state = np.stack(play['state'])\n",
    "            batch_size = 1\n",
    "            playerId = np.array(play.loc['playerId']).reshape(batch_size, 1)\n",
    "            \n",
    "            numpy_reshaped_state = np.stack(play['state'])[:,[3,4,7,8,11,12,13,14]].reshape(batch_size, -1)\n",
    "            numpy_with_playerid = np.append(numpy_reshaped_state, playerId, axis=1)\n",
    "            true_state = torch.tensor(numpy_with_playerid, dtype=torch.float32).to(device=device)\n",
    "\n",
    "            numpy_reshaped_next_state = np.stack(play['next_state'])[:,[3,4,7,8,11,12,13,14]].reshape(batch_size, -1)\n",
    "            #numpy_next_with_playerid = np.append(numpy_reshaped_next_state, playerId, axis=1)\n",
    "            true_next_state = torch.tensor(numpy_reshaped_next_state, dtype=torch.float32).to(device=device)\n",
    "            \n",
    "            model.eval()  # put model to eval mode\n",
    "\n",
    "            #compute prediction and loss\n",
    "            predicted_next_state = model(true_state)\n",
    "  \n",
    "            loss = loss_fn(predicted_next_state, true_next_state)\n",
    "            \n",
    "            test_loss += loss\n",
    "        \n",
    "        test_loss = test_loss/size\n",
    "\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "134032b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num batches: 4202\n",
      "At epoch 0, iter 0: train loss = 20935.59765625\n",
      "At epoch 0, iter 0: val loss = 259201.59375\n",
      "At epoch 0, iter 1400: train loss = 0.2765701711177826\n",
      "At epoch 0, iter 1400: val loss = 8.051009178161621\n",
      "At epoch 0, iter 2800: train loss = 0.1845938265323639\n",
      "At epoch 0, iter 2800: val loss = 6.21742582321167\n",
      "At epoch 0, iter 4200: train loss = 0.21624422073364258\n",
      "At epoch 0, iter 4200: val loss = 6.234797954559326\n",
      "post scheduler step lr =  0.0025\n",
      "At epoch 1, iter 0: train loss = 0.2914848327636719\n",
      "At epoch 1, iter 0: val loss = 6.036000728607178\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2646/2324581532.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m#print(g['lr'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loop_large\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mval_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"At epoch {k}, iter {i}: train loss = {train_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2646/3500778715.py\u001b[0m in \u001b[0;36mtest_loop_large\u001b[0;34m(test_df, model, loss_fn, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mnumpy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mis_label_like\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m   2464\u001b[0m     \"\"\"\n\u001b[1;32m   2465\u001b[0m     \u001b[0;31m# select a label or row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2466\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "epochs = 10\n",
    "training_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "num_batches = int(len(train_data)/BATCH_SIZE)+1\n",
    "\n",
    "print(f\"Num batches: {num_batches}\")\n",
    "\n",
    "indexes = list(range(0,len(train_data)))\n",
    "\n",
    "np.random.seed(2430)\n",
    "\n",
    "for k in range(epochs):\n",
    "    \n",
    "    random.shuffle(indexes)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "\n",
    "        start_index = i*BATCH_SIZE\n",
    "        end_index = min(len(train_data), (i+1)*BATCH_SIZE)\n",
    "        \n",
    "        #indices = list(range(start_index, end_index))\n",
    "        indices = indexes[start_index:end_index]\n",
    "\n",
    "        train_loss = train_loop_large(train_data, indices, cnn_model, mse_loss_fn, cnn_optimizer, device)\n",
    "        \n",
    "        training_loss_list.append(train_loss)\n",
    "\n",
    "        if (i % int(num_batches/3) == 0):\n",
    "        #if (i % int(num_batches) == 0):# and (i != 0):\n",
    "            #for g in cnn_optimizer.param_groups:\n",
    "                #g['lr'] = 0.001\n",
    "                #print(g['lr'])\n",
    "\n",
    "            val_loss = test_loop_large(val_data, cnn_model, F.mse_loss, device)\n",
    "            val_loss_list.append(val_loss)\n",
    "            print(f\"At epoch {k}, iter {i}: train loss = {train_loss}\")\n",
    "            print(f\"At epoch {k}, iter {i}: val loss = {val_loss}\")\n",
    "    \n",
    "    for g in cnn_optimizer.param_groups:\n",
    "        g['lr'] /= 2\n",
    "        print('post scheduler step lr = ', g['lr'])\n",
    "            \n",
    "    #if k > 20 and k % 5 == 0:\n",
    "    #    for g in cnn_optimizer.param_groups:\n",
    "    #        #g['lr'] = 0.001\n",
    "    #        g['lr'] /= 2\n",
    "    #        print('post scheduler step lr = ', g['lr'])\n",
    "\n",
    "    #scheduler.step()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3f6a8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(cnn_model.state_dict(), 'full_dataset_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abf33d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss  tensor(0.5202)\n"
     ]
    }
   ],
   "source": [
    "def test_baseline(test_df, loss_fn):\n",
    "    size = len(test_df)\n",
    "    test_loss = 0\n",
    "\n",
    "    for row_index in range(0,len(test_df)):\n",
    "        \n",
    "        play = test_df.iloc[row_index, :]\n",
    "\n",
    "        true_state = torch.tensor(play['state'][:,[3,4,7,8,11,12,13,14]].reshape(-1), dtype=torch.float32)\n",
    "        true_next_state = torch.tensor(play['next_state'][:,[3,4,7,8,11,12,13,14]].reshape(-1), dtype=torch.float32)\n",
    "\n",
    "        loss = loss_fn(true_next_state, true_state)\n",
    "\n",
    "        test_loss += loss\n",
    "\n",
    "    test_loss = test_loss/size\n",
    "    \n",
    "    return test_loss\n",
    "\n",
    "val_loss = test_baseline(val_data, F.mse_loss)\n",
    "print('val loss ', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a8c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728908a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d80c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
