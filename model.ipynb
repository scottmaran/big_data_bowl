{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7392d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343dee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76574a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "'''\n",
    "Actor\n",
    "Input: states\n",
    "Output: action \n",
    "'''\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, action_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        lay_out = self.linear1(state)\n",
    "        layer1_output = self.relu(lay_out)\n",
    "        layer2_output = self.relu(self.linear2(layer1_output))\n",
    "        output = self.relu(self.linear3(layer2_output))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "'''\n",
    "Critic\n",
    "Input: state, action pair\n",
    "Output: (value)\n",
    "'''\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_size+action_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        layer1_output = self.relu(self.linear1(inp))\n",
    "        layer2_output = self.relu(self.linear2(layer1_output))\n",
    "        output = self.relu(self.linear3(layer2_output))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551ec320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playIndex</th>\n",
       "      <th>timeIndex</th>\n",
       "      <th>state</th>\n",
       "      <th>next_state</th>\n",
       "      <th>reward</th>\n",
       "      <th>action</th>\n",
       "      <th>next_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>[11.4, 42.67, 12.42, 42.51, 21.17, 43.14, 20.0...</td>\n",
       "      <td>[11.39, 42.66, 11.58, 42.54, 21.24, 43.33, 20....</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[0.01, 0.01]</td>\n",
       "      <td>[-0.02, 0.03]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>[11.39, 42.66, 11.58, 42.54, 21.24, 43.33, 20....</td>\n",
       "      <td>[11.41, 42.63, 11.3, 42.45, 21.34, 43.51, 20.3...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>[-0.02, 0.03]</td>\n",
       "      <td>[-0.04, 0.07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>[11.41, 42.63, 11.3, 42.45, 21.34, 43.51, 20.3...</td>\n",
       "      <td>[11.45, 42.56, 11.32, 42.34, 21.48, 43.67, 20....</td>\n",
       "      <td>0.08</td>\n",
       "      <td>[-0.04, 0.07]</td>\n",
       "      <td>[-0.08, 0.11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>[11.45, 42.56, 11.32, 42.34, 21.48, 43.67, 20....</td>\n",
       "      <td>[11.53, 42.45, 11.15, 42.28, 21.67, 43.81, 20....</td>\n",
       "      <td>0.14</td>\n",
       "      <td>[-0.08, 0.11]</td>\n",
       "      <td>[-0.09, 0.13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>[11.53, 42.45, 11.15, 42.28, 21.67, 43.81, 20....</td>\n",
       "      <td>[11.62, 42.32, 11.51, 42.1, 21.88, 43.92, 21.2...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>[-0.09, 0.13]</td>\n",
       "      <td>[-0.16, 0.16]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   playIndex  timeIndex                                              state  \\\n",
       "0          0         48  [11.4, 42.67, 12.42, 42.51, 21.17, 43.14, 20.0...   \n",
       "1          0         49  [11.39, 42.66, 11.58, 42.54, 21.24, 43.33, 20....   \n",
       "2          0         50  [11.41, 42.63, 11.3, 42.45, 21.34, 43.51, 20.3...   \n",
       "3          0         51  [11.45, 42.56, 11.32, 42.34, 21.48, 43.67, 20....   \n",
       "4          0         52  [11.53, 42.45, 11.15, 42.28, 21.67, 43.81, 20....   \n",
       "\n",
       "                                          next_state  reward         action  \\\n",
       "0  [11.39, 42.66, 11.58, 42.54, 21.24, 43.33, 20....    0.01   [0.01, 0.01]   \n",
       "1  [11.41, 42.63, 11.3, 42.45, 21.34, 43.51, 20.3...    0.04  [-0.02, 0.03]   \n",
       "2  [11.45, 42.56, 11.32, 42.34, 21.48, 43.67, 20....    0.08  [-0.04, 0.07]   \n",
       "3  [11.53, 42.45, 11.15, 42.28, 21.67, 43.81, 20....    0.14  [-0.08, 0.11]   \n",
       "4  [11.62, 42.32, 11.51, 42.1, 21.88, 43.92, 21.2...    0.16  [-0.09, 0.13]   \n",
       "\n",
       "     next_action  \n",
       "0  [-0.02, 0.03]  \n",
       "1  [-0.04, 0.07]  \n",
       "2  [-0.08, 0.11]  \n",
       "3  [-0.09, 0.13]  \n",
       "4  [-0.16, 0.16]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('all_players.pkl')\n",
    "\n",
    "df.loc[:, 'next_action'] = df.groupby('playIndex').action.shift(-1)\n",
    "\n",
    "# drop na if using sarsa model\n",
    "sarsa_df = df.dropna(axis=0, how='any', inplace=False)\n",
    "\n",
    "sarsa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c7a7501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amans/.local/lib/python3.9/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ef886c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return batch_size of plays\n",
    "def sample_from_plays(batch_size, df):\n",
    "    \n",
    "    indices = np.random.choice(len(df), batch_size, replace=False)\n",
    "    \n",
    "    plays = df.iloc[indices, :]\n",
    "    \n",
    "    return plays\n",
    "\n",
    "'''\n",
    "Function that splits the data into a training, validation, and test set\n",
    "'''\n",
    "def split_data(dataset, train_split, seed):\n",
    "    np.random.seed(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_num = int(len(dataset)*train_split)\n",
    "    val_num = (len(dataset) - int(len(dataset)*train_split))//2\n",
    "\n",
    "    train_indices = indices[0:train_num]\n",
    "    val_indices = indices[train_num:train_num+val_num]\n",
    "    test_indices = indices[train_num+val_num:]\n",
    "\n",
    "    #check to make sure slices correct\n",
    "    assert len(dataset) == len(train_indices) + len(val_indices) + len(test_indices)\n",
    "\n",
    "    #dataset = help.normalize(train_indices, dataset)\n",
    "\n",
    "    train_data = dataset.loc[train_indices,:]\n",
    "    val_data = dataset.loc[val_indices,:]\n",
    "    test_data = dataset.loc[test_indices,:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f95767cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 46\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "iterations = 5\n",
    "state_size = len(df.loc[0,'state'])\n",
    "action_size = 2\n",
    "gamma = 0.99\n",
    "\n",
    "# create models\n",
    "critic = Critic(state_size, action_size).to(device=device)\n",
    "actor = Actor(state_size, action_size).to(device=device)\n",
    "critic_target = critic.to(device=device)\n",
    "actor_target = actor.to(device=device)\n",
    "\n",
    "# define loss function\n",
    "critic_loss_function = nn.MSELoss()\n",
    "# define optimizers\n",
    "actor_optimizer = optim.Adam(actor.parameters())\n",
    "critic_optimizer = optim.Adam(critic.parameters())\n",
    "\n",
    "print(f\"State size: {state_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a6858f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['playIndex', 'timeIndex', 'state', 'next_state', 'reward', 'action'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = split_data(df, 0.7, 2430)\n",
    "\n",
    "print(test_data.columns)\n",
    "#print(test_data.head())\n",
    "#print(test_data[test_data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b16e958f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor name, param in actor.named_parameters():\\n    if param.requires_grad:\\n        print(name, param.data)\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for name, param in actor.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "892c71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for k in range(iterations):\n",
    "    \n",
    "    play = sample_from_plays(4, train_data)\n",
    "\n",
    "    state = torch.tensor(np.stack(play['state'].values), dtype=torch.float32).to(device=device)\n",
    "    next_state = torch.tensor(np.stack(play['next_state'].values), dtype=torch.float32).to(device=device)\n",
    "    reward = torch.tensor(play['reward'].values).to(device=device)\n",
    "    true_action = torch.tensor(np.stack(play['action'].values), dtype=torch.float32).to(device=device)\n",
    "\n",
    "    actor_predicted_states = actor_target(state)\n",
    "    critic_input_from_actor = torch.cat((next_state, actor_states), 1)\n",
    "\n",
    "    # set y values\n",
    "    y = reward + gamma*critic_target(critic_input_from_actor)\n",
    "    y = y.to(torch.float32)\n",
    "\n",
    "    #\n",
    "    critic_input_true_actions = torch.cat((next_state, true_action), 1)\n",
    "    true_y = critic(critic_input_true_actions)\n",
    "\n",
    "    # update critic\n",
    "    critic.zero_grad()\n",
    "    critic_loss = critic_loss_function(y, true_y)\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    # update actor\n",
    "    policy_loss = -critic(torch.cat((state, actor(state)),1))\n",
    "    policy_loss = policy_loss.mean()\n",
    "    policy_loss.backward()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "48db2daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-76-18be44ebe60f>:23: UserWarning: Using a target size (torch.Size([4, 46])) that is different to the input size (torch.Size([4, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  individual_loss = loss_fn(actor_states, next_state)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (46) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-18be44ebe60f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mavg_test_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-18be44ebe60f>\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(test_df, actor_model, loss_fn, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mindividual_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mindividual_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/scott/comic_env/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2926\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2928\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2929\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/scott/comic_env/lib/python3.9/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (46) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Define metrics\n",
    "'''\n",
    "\n",
    "\n",
    "def test_loop(test_df, actor_model, loss_fn, device):\n",
    "    size = len(test_df)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for row in test_df[test_df.columns]:\n",
    "            \n",
    "            state = torch.tensor(np.stack(play['state'].values), dtype=torch.float32).to(device=device)\n",
    "            next_state = torch.tensor(np.stack(play['next_state'].values), dtype=torch.float32).to(device=device)\n",
    "            reward = torch.tensor(play['reward'].values).to(device=device)\n",
    "            true_action = torch.tensor(np.stack(play['action'].values), dtype=torch.float32).to(device=device)\n",
    "\n",
    "            actor_states = actor_target(state)\n",
    "            \n",
    "            print(actor_states.shape)\n",
    "            print(true_action.shape)\n",
    "            \n",
    "            individual_loss = loss_fn(actor_states, next_state)\n",
    "\n",
    "            test_loss += individual_loss\n",
    "\n",
    "        test_loss /= size\n",
    "        correct /= size\n",
    "        print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return test_loss\n",
    "        \n",
    "avg_test_loss = test_loop(test_data, actor, F.mse_loss, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4df5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd8adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fa50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#####################\n",
    "## Test code\n",
    "#####################\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in range(iterations):\n",
    "    \n",
    "    play = sample_from_plays(4, train_data)\n",
    "\n",
    "    state = torch.tensor(np.stack(play['state'].values), dtype=torch.float32).to(device=device)\n",
    "    next_state = torch.tensor(np.stack(play['next_state'].values), dtype=torch.float32).to(device=device)\n",
    "    reward = torch.tensor(play['reward'].values).to(device=device)\n",
    "    true_action = torch.tensor(np.stack(play['action'].values), dtype=torch.float32).to(device=device)\n",
    "\n",
    "    actor_states = actor_target(state)\n",
    "    critic_input_from_actor = torch.cat((next_state, actor_states), 1)\n",
    "\n",
    "    # set y values\n",
    "    y = reward + gamma*critic_target(critic_input_from_actor)\n",
    "    y = y.to(torch.float32)\n",
    "\n",
    "    #\n",
    "    critic_input_true_actions = torch.cat((next_state, true_action), 1)\n",
    "    true_y = critic(critic_input_true_actions)\n",
    "\n",
    "    # update critic\n",
    "    critic.zero_grad()\n",
    "    critic_loss = critic_loss_function(y, true_y)\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    # update actor\n",
    "    policy_loss = -critic(torch.cat((state, actor(state)),1))\n",
    "    policy_loss = policy_loss.mean()\n",
    "    policy_loss.backward()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fefa311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playIndex</th>\n",
       "      <th>timeIndex</th>\n",
       "      <th>state</th>\n",
       "      <th>next_state</th>\n",
       "      <th>reward</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>[11.4, 42.67, 55.74, 36.49, 47.81, 23.35, 47.3...</td>\n",
       "      <td>[11.39, 42.66, 55.07, 36.68, 46.88, 23.75, 46....</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[0.01, 0.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>[11.39, 42.66, 55.07, 36.68, 46.88, 23.75, 46....</td>\n",
       "      <td>[11.41, 42.63, 54.4, 36.86, 45.96, 24.14, 45.6...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>[-0.02, 0.03]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>[11.41, 42.63, 54.4, 36.86, 45.96, 24.14, 45.6...</td>\n",
       "      <td>[11.45, 42.56, 53.73, 37.04, 45.04, 24.53, 44....</td>\n",
       "      <td>0.08</td>\n",
       "      <td>[-0.04, 0.07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>[11.45, 42.56, 53.73, 37.04, 45.04, 24.53, 44....</td>\n",
       "      <td>[11.53, 42.45, 53.07, 37.21, 44.11, 24.92, 43....</td>\n",
       "      <td>0.14</td>\n",
       "      <td>[-0.08, 0.11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>[11.53, 42.45, 53.07, 37.21, 44.11, 24.92, 43....</td>\n",
       "      <td>[11.62, 42.32, 52.41, 37.37, 43.17, 25.29, 43....</td>\n",
       "      <td>0.16</td>\n",
       "      <td>[-0.09, 0.13]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   playIndex  timeIndex                                              state  \\\n",
       "0          0         48  [11.4, 42.67, 55.74, 36.49, 47.81, 23.35, 47.3...   \n",
       "1          0         49  [11.39, 42.66, 55.07, 36.68, 46.88, 23.75, 46....   \n",
       "2          0         50  [11.41, 42.63, 54.4, 36.86, 45.96, 24.14, 45.6...   \n",
       "3          0         51  [11.45, 42.56, 53.73, 37.04, 45.04, 24.53, 44....   \n",
       "4          0         52  [11.53, 42.45, 53.07, 37.21, 44.11, 24.92, 43....   \n",
       "\n",
       "                                          next_state  reward         action  \n",
       "0  [11.39, 42.66, 55.07, 36.68, 46.88, 23.75, 46....    0.01   [0.01, 0.01]  \n",
       "1  [11.41, 42.63, 54.4, 36.86, 45.96, 24.14, 45.6...    0.04  [-0.02, 0.03]  \n",
       "2  [11.45, 42.56, 53.73, 37.04, 45.04, 24.53, 44....    0.08  [-0.04, 0.07]  \n",
       "3  [11.53, 42.45, 53.07, 37.21, 44.11, 24.92, 43....    0.14  [-0.08, 0.11]  \n",
       "4  [11.62, 42.32, 52.41, 37.37, 43.17, 25.29, 43....    0.16  [-0.09, 0.13]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't need buffer because have all states\n",
    "import ast\n",
    "\n",
    "def from_np_array(array_string):\n",
    "    array_string = ','.join(array_string.replace('[ ', '[').split())\n",
    "    index = 0\n",
    "    #array_string = ','.join(array_string.replace('\\n', ' ').split())\n",
    "    try:\n",
    "        return np.array(ast.literal_eval(array_string))\n",
    "    except:\n",
    "        index += 1\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"all_players.csv\", index_col=0, converters={'state':from_np_array, 'next_state':from_np_array, 'action':from_np_array})\n",
    "\n",
    "df.head()\n",
    "\n",
    "#from_np_array(df['state'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "335b7f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor states: tensor([[0.0000, 7.6483],\n",
      "        [0.0000, 6.5292]], grad_fn=<ReluBackward0>)\n",
      "Critic targets: tensor([[3.4059],\n",
      "        [3.6254]], grad_fn=<ReluBackward0>)\n",
      "torch.Size([2])\n",
      "torch.Size([2, 1])\n",
      "tensor([[3.9719, 4.0819],\n",
      "        [4.1891, 4.2991]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 46])\n",
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-3.4946, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_states = actor_target(next_state)\n",
    "\n",
    "print(f\"Actor states: {actor_states}\")\n",
    "\n",
    "# concat state (N, X) and action (N, Y)\n",
    "inp = torch.cat((next_state, actor_states), 1)\n",
    "\n",
    "ta = critic_target(inp)\n",
    "\n",
    "print(f\"Critic targets: {ta}\")\n",
    "\n",
    "print(reward.shape)\n",
    "print(ta.shape)\n",
    "\n",
    "y = reward + gamma*ta\n",
    "print(y)\n",
    "\n",
    "print(state.shape)\n",
    "print(actor(state).shape)\n",
    "\n",
    "-critic(torch.cat((state, actor(state)),1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2afcb0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 88.2000,   4.6300,  84.4500,   6.4400,  82.7200,  14.6000,  87.4600,\n",
      "           4.1400,  91.2100,  16.9700,  84.9500,  18.2500,  87.3600,  21.5200,\n",
      "          89.0500,  15.4500,  64.7300,  12.2100,  74.0600,  27.6600,  88.8400,\n",
      "          13.4600,  88.0400,   6.0000,  88.6000,  22.1000,  87.9500,   7.1700,\n",
      "          87.3100,   5.5000,  89.7500,  13.2300,  84.8300,   7.1400,  89.0600,\n",
      "          19.7400,  92.1000,  22.9200,  82.8900,   6.9900,  88.8600,   6.9700,\n",
      "          88.8600,   9.5400,  88.1400,   4.6600],\n",
      "        [102.2200,  13.6100,  66.9300,  19.8600,  80.6100,  19.3300,  84.1500,\n",
      "          28.1300,  83.7600,   8.2300,  83.6500,  16.9700,  82.7500,  21.3900,\n",
      "          84.4700,  35.4800,  78.6300,   9.7200,  82.0200,  12.1900,  82.7500,\n",
      "          10.5500,  86.0800,  24.4400,  75.7200,  19.4500,  85.6500,  33.7300,\n",
      "          83.1700,  21.9400,  84.8100,  17.4000,  88.8100,  10.3300,  85.1000,\n",
      "          12.2300,  87.3000,  24.1100,  82.9300,  18.6300,  82.9200,  18.2300,\n",
      "          88.4000,  26.2800, 101.8800,  13.2400]], dtype=torch.float64)\n",
      "################\n",
      "tensor([[ 88.2000,   4.6300,  84.4500,   6.4400,  82.7200,  14.6000,  87.4600,\n",
      "           4.1400,  91.2100,  16.9700,  84.9500,  18.2500,  87.3600,  21.5200,\n",
      "          89.0500,  15.4500,  64.7300,  12.2100,  74.0600,  27.6600,  88.8400,\n",
      "          13.4600,  88.0400,   6.0000,  88.6000,  22.1000,  87.9500,   7.1700,\n",
      "          87.3100,   5.5000,  89.7500,  13.2300,  84.8300,   7.1400,  89.0600,\n",
      "          19.7400,  92.1000,  22.9200,  82.8900,   6.9900,  88.8600,   6.9700,\n",
      "          88.8600,   9.5400,  88.1400,   4.6600],\n",
      "        [102.2200,  13.6100,  66.9300,  19.8600,  80.6100,  19.3300,  84.1500,\n",
      "          28.1300,  83.7600,   8.2300,  83.6500,  16.9700,  82.7500,  21.3900,\n",
      "          84.4700,  35.4800,  78.6300,   9.7200,  82.0200,  12.1900,  82.7500,\n",
      "          10.5500,  86.0800,  24.4400,  75.7200,  19.4500,  85.6500,  33.7300,\n",
      "          83.1700,  21.9400,  84.8100,  17.4000,  88.8100,  10.3300,  85.1000,\n",
      "          12.2300,  87.3000,  24.1100,  82.9300,  18.6300,  82.9200,  18.2300,\n",
      "          88.4000,  26.2800, 101.8800,  13.2400]])\n",
      "tensor([[ 87.7600,   4.2200,  84.8100,   6.3300,  82.9900,  14.1900,  87.8600,\n",
      "           3.9700,  91.0500,  16.6700,  84.8900,  17.7300,  86.9900,  21.1500,\n",
      "          88.5800,  15.1500,  65.1800,  11.8200,  74.3800,  27.5800,  88.8900,\n",
      "          13.2700,  87.9000,   5.4900,  88.7500,  21.6800,  88.0300,   6.6300,\n",
      "          87.0200,   5.0600,  89.8100,  12.9200,  84.7200,   6.7100,  89.5600,\n",
      "          19.5800,  91.7800,  22.7200,  83.1900,   6.4800,  88.7700,   6.4000,\n",
      "          88.3000,   9.1800,  87.6800,   4.3200],\n",
      "        [101.5400,  13.3900,  67.4200,  19.5500,  80.3200,  19.0300,  84.6300,\n",
      "          27.6300,  84.3300,   8.2800,  83.9700,  16.8400,  83.2400,  20.8500,\n",
      "          84.8900,  35.0700,  79.1400,   9.7700,  82.2900,  12.4300,  83.2700,\n",
      "          10.5600,  86.4900,  23.8400,  75.8100,  19.1100,  85.8800,  33.6900,\n",
      "          83.4400,  21.4500,  85.2600,  17.4000,  88.4700,  10.1300,  85.2600,\n",
      "          12.0700,  87.3600,  23.7100,  83.1700,  18.6600,  83.1800,  18.2100,\n",
      "          88.1000,  26.0400, 101.2400,  12.9900]])\n",
      "tensor([0.6000, 0.7100], dtype=torch.float64)\n",
      "tensor([[0.4400, 0.4100],\n",
      "        [0.6800, 0.2200]])\n",
      "####################\n",
      "torch.Size([2, 46])\n",
      "torch.Size([2, 2])\n",
      "tensor([[ 88.2000,   4.6300,  84.4500,   6.4400,  82.7200,  14.6000,  87.4600,\n",
      "           4.1400,  91.2100,  16.9700,  84.9500,  18.2500,  87.3600,  21.5200,\n",
      "          89.0500,  15.4500,  64.7300,  12.2100,  74.0600,  27.6600,  88.8400,\n",
      "          13.4600,  88.0400,   6.0000,  88.6000,  22.1000,  87.9500,   7.1700,\n",
      "          87.3100,   5.5000,  89.7500,  13.2300,  84.8300,   7.1400,  89.0600,\n",
      "          19.7400,  92.1000,  22.9200,  82.8900,   6.9900,  88.8600,   6.9700,\n",
      "          88.8600,   9.5400,  88.1400,   4.6600,   0.4400,   0.4100],\n",
      "        [102.2200,  13.6100,  66.9300,  19.8600,  80.6100,  19.3300,  84.1500,\n",
      "          28.1300,  83.7600,   8.2300,  83.6500,  16.9700,  82.7500,  21.3900,\n",
      "          84.4700,  35.4800,  78.6300,   9.7200,  82.0200,  12.1900,  82.7500,\n",
      "          10.5500,  86.0800,  24.4400,  75.7200,  19.4500,  85.6500,  33.7300,\n",
      "          83.1700,  21.9400,  84.8100,  17.4000,  88.8100,  10.3300,  85.1000,\n",
      "          12.2300,  87.3000,  24.1100,  82.9300,  18.6300,  82.9200,  18.2300,\n",
      "          88.4000,  26.2800, 101.8800,  13.2400,   0.6800,   0.2200]])\n",
      "torch.Size([2, 48])\n"
     ]
    }
   ],
   "source": [
    "play = sample_from_plays(2, df)\n",
    "\n",
    "print(torch.tensor(np.stack(play['state'].values)))\n",
    "\n",
    "print(\"################\")\n",
    "\n",
    "state = torch.tensor(np.stack(play['state'].values), dtype=torch.float32)\n",
    "next_state = torch.tensor(np.stack(play['next_state'].values), dtype=torch.float32)\n",
    "reward = torch.tensor(play['reward'].values)\n",
    "true_action = torch.tensor(np.stack(play['action'].values), dtype=torch.float32)\n",
    "\n",
    "# print(play['action'].values)\n",
    "# print(torch.tensor(play['action']))\n",
    "\n",
    "print(state)\n",
    "print(next_state)\n",
    "print(reward)\n",
    "print(true_action)\n",
    "print(\"####################\")\n",
    "\n",
    "print(state.size())\n",
    "print(true_action.size())\n",
    "cat =torch.cat((state,true_action), 1)\n",
    "print(cat)\n",
    "print(cat.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf47f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9ea68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acb3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
