{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7a8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a88a84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playIndex</th>\n",
       "      <th>timeIndex</th>\n",
       "      <th>state</th>\n",
       "      <th>next_state</th>\n",
       "      <th>reward</th>\n",
       "      <th>action</th>\n",
       "      <th>next_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>[11.4, 42.67, 12.42, 42.51, 21.17, 43.14, 20.0...</td>\n",
       "      <td>[11.39, 42.66, 11.58, 42.54, 21.24, 43.33, 20....</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[0.01, 0.01]</td>\n",
       "      <td>[-0.02, 0.03]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>[11.39, 42.66, 11.58, 42.54, 21.24, 43.33, 20....</td>\n",
       "      <td>[11.41, 42.63, 11.3, 42.45, 21.34, 43.51, 20.3...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>[-0.02, 0.03]</td>\n",
       "      <td>[-0.04, 0.07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>[11.41, 42.63, 11.3, 42.45, 21.34, 43.51, 20.3...</td>\n",
       "      <td>[11.45, 42.56, 11.32, 42.34, 21.48, 43.67, 20....</td>\n",
       "      <td>0.08</td>\n",
       "      <td>[-0.04, 0.07]</td>\n",
       "      <td>[-0.08, 0.11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>[11.45, 42.56, 11.32, 42.34, 21.48, 43.67, 20....</td>\n",
       "      <td>[11.53, 42.45, 11.15, 42.28, 21.67, 43.81, 20....</td>\n",
       "      <td>0.14</td>\n",
       "      <td>[-0.08, 0.11]</td>\n",
       "      <td>[-0.09, 0.13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>[11.53, 42.45, 11.15, 42.28, 21.67, 43.81, 20....</td>\n",
       "      <td>[11.62, 42.32, 11.51, 42.1, 21.88, 43.92, 21.2...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>[-0.09, 0.13]</td>\n",
       "      <td>[-0.16, 0.16]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   playIndex  timeIndex                                              state  \\\n",
       "0          0         48  [11.4, 42.67, 12.42, 42.51, 21.17, 43.14, 20.0...   \n",
       "1          0         49  [11.39, 42.66, 11.58, 42.54, 21.24, 43.33, 20....   \n",
       "2          0         50  [11.41, 42.63, 11.3, 42.45, 21.34, 43.51, 20.3...   \n",
       "3          0         51  [11.45, 42.56, 11.32, 42.34, 21.48, 43.67, 20....   \n",
       "4          0         52  [11.53, 42.45, 11.15, 42.28, 21.67, 43.81, 20....   \n",
       "\n",
       "                                          next_state  reward         action  \\\n",
       "0  [11.39, 42.66, 11.58, 42.54, 21.24, 43.33, 20....    0.01   [0.01, 0.01]   \n",
       "1  [11.41, 42.63, 11.3, 42.45, 21.34, 43.51, 20.3...    0.04  [-0.02, 0.03]   \n",
       "2  [11.45, 42.56, 11.32, 42.34, 21.48, 43.67, 20....    0.08  [-0.04, 0.07]   \n",
       "3  [11.53, 42.45, 11.15, 42.28, 21.67, 43.81, 20....    0.14  [-0.08, 0.11]   \n",
       "4  [11.62, 42.32, 11.51, 42.1, 21.88, 43.92, 21.2...    0.16  [-0.09, 0.13]   \n",
       "\n",
       "     next_action  \n",
       "0  [-0.02, 0.03]  \n",
       "1  [-0.04, 0.07]  \n",
       "2  [-0.08, 0.11]  \n",
       "3  [-0.09, 0.13]  \n",
       "4  [-0.16, 0.16]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_pickle('all_players.pkl')\n",
    "\n",
    "data_df.loc[:, 'next_action'] = data_df.groupby('playIndex').action.shift(-1)\n",
    "\n",
    "# drop na if using sarsa model\n",
    "sarsa_df = data_df.dropna(axis=0, how='any', inplace=False)\n",
    "\n",
    "sarsa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5173de50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204941"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sarsa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a149c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sarsa_df.loc[1:2, 'state']\n",
    "\n",
    "stacked = np.stack(x.values)\n",
    "\n",
    "dl = torch.utils.data.DataLoader(sarsa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c3176b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return batch_size of plays\n",
    "def sample_from_plays(batch_size, df, seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    indices = np.random.choice(len(df), batch_size, replace=False)\n",
    "    \n",
    "    plays = df.iloc[indices, :]\n",
    "    \n",
    "    return plays\n",
    "\n",
    "'''\n",
    "Function that splits the data into a training, validation, and test set\n",
    "'''\n",
    "def split_data(dataset, train_split, seed):\n",
    "    np.random.seed(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_num = int(len(dataset)*train_split)\n",
    "    val_num = (len(dataset) - int(len(dataset)*train_split))//2\n",
    "\n",
    "    train_indices = indices[0:train_num]\n",
    "    val_indices = indices[train_num:train_num+val_num]\n",
    "    test_indices = indices[train_num+val_num:]\n",
    "\n",
    "    #check to make sure slices correct\n",
    "    assert len(dataset) == len(train_indices) + len(val_indices) + len(test_indices)\n",
    "\n",
    "    #dataset = help.normalize(train_indices, dataset)\n",
    "\n",
    "    train_data = dataset.iloc[train_indices,:]\n",
    "    val_data = dataset.iloc[val_indices,:]\n",
    "    test_data = dataset.iloc[test_indices,:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e8958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['playIndex', 'timeIndex', 'state', 'next_state', 'reward', 'action',\n",
      "       'next_action'],\n",
      "      dtype='object')\n",
      "cpu\n",
      "Length of training data: 143458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amans/Development/scott/comic_env/lib/python3.9/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_data, val_data, test_data = split_data(sarsa_df, 0.7, 2430)\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "#val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "#test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "print(test_data.columns)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)\n",
    "\n",
    "print(f\"Length of training data: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "062b5a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 46\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "iterations = 25000\n",
    "state_size = len(data_df.loc[0,'state'])\n",
    "action_size = 2\n",
    "gamma = 0.99\n",
    "COPY_TARGETS_INDEX = 5\n",
    "\n",
    "# create models\n",
    "eval_net = Qnet(state_size, action_size).to(device=device)\n",
    "target_net = type(eval_net)(state_size, action_size).to(device=device)\n",
    "target_net.load_state_dict(eval_net.state_dict())\n",
    "\n",
    "# define loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "# define optimizers\n",
    "optimizer = optim.Adam(eval_net.parameters())\n",
    "\n",
    "print(f\"State size: {state_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0513b3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Qnet(nn.Module):\\n    def __init__(self, state_size, action_size):\\n        super(Qnet, self).__init__()\\n        self.state_size = state_size\\n        self.action_size = action_size\\n        \\n        self.linear1 = nn.Linear(state_size+action_size, 128)\\n        self.linear2 = nn.Linear(128, 256)\\n        self.linear3 = nn.Linear(256, 1)\\n        \\n        self.relu = nn.ReLU()\\n        \\n    def forward(self, inp):\\n        \\n        layer1_output = self.relu(self.linear1(inp))\\n        layer2_output = self.relu(self.linear2(layer1_output))\\n        output = self.linear3(layer2_output)\\n        \\n        return output\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Neural Network that will represent the Q-function\n",
    "Input: concatenated (state,action) pair\n",
    "Output: value\n",
    "'''\n",
    "\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_size+action_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 128)\n",
    "        self.linear4 = nn.Linear(128,1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        layer1_output = self.relu(self.linear1(inp))\n",
    "        layer2_output = self.relu(self.linear2(layer1_output))\n",
    "        layer3_output = self.relu(self.linear3(layer2_output))\n",
    "        output = self.linear4(layer3_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "'''\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_size+action_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        layer1_output = self.relu(self.linear1(inp))\n",
    "        layer2_output = self.relu(self.linear2(layer1_output))\n",
    "        output = self.linear3(layer2_output)\n",
    "        \n",
    "        return output\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0e5a4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch, labels in train_loader:\n",
    "#    print(batch)\n",
    "#    print(labels)\n",
    "    \n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ab9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_loop(test_df, q_model, test_loss_fn, device):\n",
    "    size = len(test_df)\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for row_index in range(0,len(test_df)):\n",
    "            #try:\n",
    "            row = test_df.iloc[row_index, :]\n",
    "\n",
    "            state = torch.tensor(row['state'], dtype=torch.float32).to(device=device)\n",
    "            action = torch.tensor(row['action'], dtype=torch.float32).to(device=device)\n",
    "            reward = torch.tensor(row['reward'], dtype=torch.float32).to(device=device)\n",
    "            next_state = torch.tensor(row['next_state'], dtype=torch.float32).to(device=device)\n",
    "            next_action = torch.tensor(row['next_action'], dtype=torch.float32).to(device=device)\n",
    "\n",
    "            # Update Q\n",
    "            eval_input = torch.cat((state, action), 0)\n",
    "\n",
    "            q_eval = q_model(eval_input)\n",
    "\n",
    "            target_input = torch.cat((next_state, next_action), 0)\n",
    "            q_target = reward + gamma*q_model(target_input)\n",
    "\n",
    "            loss = test_loss_fn(q_eval, q_target)\n",
    "            test_loss += loss\n",
    "            #except Exception as e:\n",
    "                #print(f\"Row index: {row_index}\")\n",
    "                #print(F\"Exception: {e}\")\n",
    "                #print(row)\n",
    "    \n",
    "        test_loss /= size\n",
    "            #print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss\n",
    "        \n",
    "#avg_test_loss = test_loop(test_data, eval_net, F.mse_loss, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1702df49",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-251396475baa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m### Use random weights as baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0muntrained_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0muntrained_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8f5a532e6d82>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_size, action_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mQnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "########\n",
    "### Random loss is .325604\n",
    "### Use random weights as baseline\n",
    "########\n",
    "untrained_net = type(eval_net)(state_size, action_size).to(device=device)\n",
    "untrained_net.load_state_dict(eval_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32eb430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num batches: 1121\n",
      "At epoch 0: avg. val loss = 17.812326431274414\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c568e775343b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/scott/comic_env/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/scott/comic_env/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 10\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "num_batches = int(len(train_data)/BATCH_SIZE)+1\n",
    "\n",
    "print(f\"Num batches: {num_batches}\")\n",
    "\n",
    "for k in range(epochs):\n",
    "    \n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        \n",
    "        \n",
    "        #warnings.simplefilter(\"error\")\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "        \n",
    "            # update weights of target network\n",
    "            if i % COPY_TARGETS_INDEX == 0:\n",
    "                target_net.load_state_dict(eval_net.state_dict())\n",
    "\n",
    "\n",
    "            #play = sample_from_plays(BATCH_SIZE, train_data, seed=k)\n",
    "            # get sample from data\n",
    "            start_index = i*BATCH_SIZE\n",
    "            end_index = min(len(train_data), (i+1)*BATCH_SIZE)\n",
    "                            \n",
    "            play = train_data.iloc[list(range(start_index, end_index)), :]\n",
    "\n",
    "            state = torch.tensor(np.stack(play['state'].values), dtype=torch.float32).to(device=device)\n",
    "            action = torch.tensor(np.stack(play['action'].values), dtype=torch.float32).to(device=device)\n",
    "            reward = torch.tensor(play['reward'].values, dtype=torch.float32).to(device=device)\n",
    "            next_state = torch.tensor(np.stack(play['next_state'].values), dtype=torch.float32).to(device=device)\n",
    "            next_action = torch.tensor(np.stack(play['next_action'].values), dtype=torch.float32).to(device=device)\n",
    "\n",
    "            # Update Q\n",
    "            eval_input = torch.cat((state, action), 1)\n",
    "            q_eval = eval_net(eval_input)\n",
    "\n",
    "            target_input = torch.cat((next_state, next_action), 1)\n",
    "            q_target = torch.unsqueeze(reward,1) + gamma*target_net(target_input)\n",
    "\n",
    "            loss = loss_fn(q_eval, q_target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % (num_batches/3) == 0:\n",
    "                avg_val_loss = test_loop(val_data, eval_net, F.mse_loss, device)\n",
    "                loss_list.append((k,avg_val_loss))\n",
    "                print(f\"At epoch {k}: avg. val loss = {avg_val_loss}\")\n",
    "        \n",
    "        if len(w):\n",
    "            print(f\"Batch index {i}\")\n",
    "            print(f\"warning: {w}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b4933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09498458",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "\n",
    "for k in range(iterations):\n",
    "    \n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        #warnings.simplefilter(\"error\")\n",
    "        \n",
    "        # update weights of target network\n",
    "        if k % COPY_TARGETS_INDEX == 0:\n",
    "            target_net.load_state_dict(eval_net.state_dict())\n",
    "\n",
    "\n",
    "        play = sample_from_plays(128, train_data, seed=k)\n",
    "\n",
    "        state = torch.tensor(np.stack(play['state'].values), dtype=torch.float32).to(device=device)\n",
    "        action = torch.tensor(np.stack(play['action'].values), dtype=torch.float32).to(device=device)\n",
    "        reward = torch.tensor(play['reward'].values, dtype=torch.float32).to(device=device)\n",
    "        next_state = torch.tensor(np.stack(play['next_state'].values), dtype=torch.float32).to(device=device)\n",
    "        next_action = torch.tensor(np.stack(play['next_action'].values), dtype=torch.float32).to(device=device)\n",
    "\n",
    "        # Update Q\n",
    "        eval_input = torch.cat((state, action), 1)\n",
    "        q_eval = eval_net(eval_input)\n",
    "\n",
    "        target_input = torch.cat((next_state, next_action), 1)\n",
    "        q_target = torch.unsqueeze(reward,1) + gamma*target_net(target_input)\n",
    "\n",
    "        loss = loss_fn(q_eval, q_target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if (k % 1000 == 0) and (k != 0):\n",
    "            avg_val_loss = test_loop(val_data, eval_net, F.mse_loss, device)\n",
    "            loss_list.append((k,avg_val_loss))\n",
    "            print(f\"At iter {k}: avg. val loss = {avg_val_loss}\")\n",
    "        \n",
    "        if len(w):\n",
    "            print(f\"row index {k}\")\n",
    "            print(f\"warning: {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e665bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fe999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2513b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7402da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786e5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8fff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c622ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f8e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
