{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdb815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70487750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playIndex</th>\n",
       "      <th>timeIndex</th>\n",
       "      <th>state</th>\n",
       "      <th>next_state</th>\n",
       "      <th>reward</th>\n",
       "      <th>action</th>\n",
       "      <th>next_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>[11.4, 42.67, 0.2, 3.07, 102.1, 312.17, 12.42,...</td>\n",
       "      <td>[11.39, 42.66, 0.21, 2.84, 105.48, 199.45, 11....</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[0.01, 0.01]</td>\n",
       "      <td>[-0.02, 0.03]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>[11.39, 42.66, 0.21, 2.84, 105.48, 199.45, 11....</td>\n",
       "      <td>[11.41, 42.63, 0.47, 2.53, 113.86, 168.36, 11....</td>\n",
       "      <td>0.04</td>\n",
       "      <td>[-0.02, 0.03]</td>\n",
       "      <td>[-0.04, 0.07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>[11.41, 42.63, 0.47, 2.53, 113.86, 168.36, 11....</td>\n",
       "      <td>[11.45, 42.56, 0.83, 2.53, 110.69, 158.32, 11....</td>\n",
       "      <td>0.08</td>\n",
       "      <td>[-0.04, 0.07]</td>\n",
       "      <td>[-0.08, 0.11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>[11.45, 42.56, 0.83, 2.53, 110.69, 158.32, 11....</td>\n",
       "      <td>[11.53, 42.45, 1.32, 3.03, 117.3, 148.06, 11.1...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>[-0.08, 0.11]</td>\n",
       "      <td>[-0.09, 0.13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>[11.53, 42.45, 1.32, 3.03, 117.3, 148.06, 11.1...</td>\n",
       "      <td>[11.62, 42.32, 1.7, 3.13, 114.42, 144.84, 11.5...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>[-0.09, 0.13]</td>\n",
       "      <td>[-0.16, 0.16]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   playIndex  timeIndex                                              state  \\\n",
       "0          0         48  [11.4, 42.67, 0.2, 3.07, 102.1, 312.17, 12.42,...   \n",
       "1          0         49  [11.39, 42.66, 0.21, 2.84, 105.48, 199.45, 11....   \n",
       "2          0         50  [11.41, 42.63, 0.47, 2.53, 113.86, 168.36, 11....   \n",
       "3          0         51  [11.45, 42.56, 0.83, 2.53, 110.69, 158.32, 11....   \n",
       "4          0         52  [11.53, 42.45, 1.32, 3.03, 117.3, 148.06, 11.1...   \n",
       "\n",
       "                                          next_state  reward         action  \\\n",
       "0  [11.39, 42.66, 0.21, 2.84, 105.48, 199.45, 11....    0.01   [0.01, 0.01]   \n",
       "1  [11.41, 42.63, 0.47, 2.53, 113.86, 168.36, 11....    0.04  [-0.02, 0.03]   \n",
       "2  [11.45, 42.56, 0.83, 2.53, 110.69, 158.32, 11....    0.08  [-0.04, 0.07]   \n",
       "3  [11.53, 42.45, 1.32, 3.03, 117.3, 148.06, 11.1...    0.14  [-0.08, 0.11]   \n",
       "4  [11.62, 42.32, 1.7, 3.13, 114.42, 144.84, 11.5...    0.16  [-0.09, 0.13]   \n",
       "\n",
       "     next_action  \n",
       "0  [-0.02, 0.03]  \n",
       "1  [-0.04, 0.07]  \n",
       "2  [-0.08, 0.11]  \n",
       "3  [-0.09, 0.13]  \n",
       "4  [-0.16, 0.16]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_pickle('datasets/all_players_rel_all_actions_group_team.pkl')\n",
    "\n",
    "data_df.loc[:, 'next_action'] = data_df.groupby('playIndex').action.shift(-1)\n",
    "\n",
    "ac_df = data_df.dropna(axis=0, how='any', inplace=False)\n",
    "\n",
    "ac_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b6ad2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [0.01647861 0.00798887]\n",
      "max: [1.59 2.34]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get max action which is used in soft AC method\n",
    "'''\n",
    "\n",
    "action_mat = np.stack(ac_df.action)\n",
    "max_action = np.max(action_mat, 0)\n",
    "mean_action = np.mean(action_mat, 0)\n",
    "print(f\"mean: {mean_action}\")\n",
    "print(f\"max: {max_action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6876f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['playIndex', 'timeIndex', 'state', 'next_state', 'reward', 'action',\n",
      "       'next_action'],\n",
      "      dtype='object')\n",
      "Length of training data: 154821\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Function that splits the data into a training, validation, and test set\n",
    "'''\n",
    "def split_data(dataset, train_split, seed):\n",
    "    np.random.seed(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_num = int(len(dataset)*train_split)\n",
    "    val_num = (len(dataset) - int(len(dataset)*train_split))//2\n",
    "\n",
    "    train_indices = indices[0:train_num]\n",
    "    val_indices = indices[train_num:train_num+val_num]\n",
    "    test_indices = indices[train_num+val_num:]\n",
    "\n",
    "    #check to make sure slices correct\n",
    "    assert len(dataset) == len(train_indices) + len(val_indices) + len(test_indices)\n",
    "\n",
    "    #dataset = help.normalize(train_indices, dataset)\n",
    "\n",
    "    train_data = dataset.iloc[train_indices,:]\n",
    "    val_data = dataset.iloc[val_indices,:]\n",
    "    test_data = dataset.iloc[test_indices,:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "train_data, val_data, test_data = split_data(ac_df, 0.7, 2430)\n",
    "\n",
    "print(test_data.columns)\n",
    "\n",
    "print(f\"Length of training data: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75372b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b143604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(test_df, update_actor, update_critic, test_loss_fn, device):\n",
    "    size = len(test_df)\n",
    "    test_q_loss = 0\n",
    "    test_a_loss = 0\n",
    "    update_actor.eval()\n",
    "    update_critic.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for row_index in range(0,len(test_df)):\n",
    "            #try:\n",
    "            row = test_df.iloc[row_index, :]\n",
    "\n",
    "            state = torch.tensor(row['state'], dtype=torch.float32).to(device=device)\n",
    "            action = torch.tensor(row['action'], dtype=torch.float32).to(device=device)\n",
    "            reward = torch.tensor(row['reward'], dtype=torch.float32).to(device=device)\n",
    "            next_state = torch.tensor(row['next_state'], dtype=torch.float32).to(device=device)\n",
    "            next_action = torch.tensor(row['next_action'], dtype=torch.float32).to(device=device)\n",
    "            \n",
    "            # do this because forget to replace nans in next state in dataset construction\n",
    "            next_state = torch.nan_to_num(next_state, nan=0)\n",
    "            #if next_state == 0:\n",
    "            #    continue\n",
    "\n",
    "            # Update Q\n",
    "            eval_input = torch.cat((state, action), 0)\n",
    "            eval_input = torch.unsqueeze(eval_input, 0)\n",
    "            q_eval = update_critic(eval_input)\n",
    "\n",
    "            target_input = torch.cat((next_state, next_action), 0)\n",
    "            target_input = torch.unsqueeze(target_input, 0)\n",
    "            q_target = reward + gamma*update_critic(target_input)\n",
    "\n",
    "            q_loss = test_loss_fn(q_eval, q_target)\n",
    "            test_q_loss += q_loss\n",
    "    \n",
    "            # get actor loss\n",
    "            actor_next_action = update_actor(torch.unsqueeze(state,0))\n",
    "            next_action = torch.unsqueeze(next_action,0)\n",
    "            a_loss = test_loss_fn(actor_next_action, next_action)\n",
    "            test_a_loss += a_loss\n",
    "    \n",
    "        test_q_loss /= size\n",
    "        test_a_loss /= size\n",
    "            #print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return [test_q_loss, test_a_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c902d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74117090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 138\n",
      "Action size: 2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Define models\n",
    "'''\n",
    "\n",
    "'''\n",
    "Input: Takes in a (state,action) pair\n",
    "Output: Outputs a Q-score\n",
    "'''\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.layer2_size = (state_size+action_size)*2\n",
    "        \n",
    "        #self.batch_initial = nn.BatchNorm1d(state_size+action_size)\n",
    "        self.batch1 = nn.BatchNorm1d(self.layer2_size)\n",
    "        self.linear1 = nn.Linear(state_size+action_size, self.layer2_size)\n",
    "        self.linear2 = nn.Linear(self.layer2_size, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        #inp = self.batch_initial(inp)\n",
    "        \n",
    "        layer1_output = self.relu(self.batch1(self.linear1(inp)))\n",
    "        output = self.linear2(layer1_output)\n",
    "        return output\n",
    "    \n",
    "'''\n",
    "Actor\n",
    "Input: states\n",
    "Output: action \n",
    "'''\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.max_action = max_action\n",
    "        \n",
    "        self.layer2_size = state_size*2\n",
    "        self.layer3_size = state_size\n",
    "        \n",
    "        self.batch_initial = nn.BatchNorm1d(state_size)\n",
    "        self.linear1 = nn.Linear(state_size, self.layer2_size)\n",
    "        self.batch1 = nn.BatchNorm1d(self.layer2_size)\n",
    "        self.linear2 = nn.Linear(self.layer2_size, self.layer3_size)\n",
    "        self.batch2 = nn.BatchNorm1d(self.layer3_size)\n",
    "        self.linear3 = nn.Linear(self.layer3_size, action_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        state = self.batch_initial(state)\n",
    "        layer1_output = self.relu(self.batch1(self.linear1(state)))\n",
    "        layer2_output = self.relu(self.batch2(self.linear2(layer1_output)))\n",
    "        output = self.linear3(layer2_output)\n",
    "        \n",
    "        output = self.max_action*torch.tanh(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "##############\n",
    "### Notes ####\n",
    "# How to use max action??\n",
    "##############\n",
    "\n",
    "    \n",
    "'''\n",
    "Define hyperparamters\n",
    "'''\n",
    "BATCH_SIZE = 128\n",
    "iterations = 25000\n",
    "state_size = len(ac_df.loc[0,'state'])\n",
    "action_size = 2\n",
    "gamma = 0.99\n",
    "max_action = torch.tensor(max_action, dtype=torch.float32).to(device=device)\n",
    "\n",
    "print(f\"State size: {state_size}\")\n",
    "print(f\"Action size: {action_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59a49001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models\n",
    "update_actor = Actor(state_size, action_size, max_action).to(device=device)\n",
    "update_critic = Critic(state_size, action_size).to(device=device)\n",
    "\n",
    "target_actor = type(update_actor)(state_size, action_size, max_action).to(device=device)\n",
    "target_actor.load_state_dict(update_actor.state_dict())\n",
    "\n",
    "target_critic = type(update_critic)(state_size, action_size).to(device=device)\n",
    "target_critic.load_state_dict(update_critic.state_dict())\n",
    "\n",
    "# define loss function\n",
    "target_loss_fn = nn.MSELoss()\n",
    "# define optimizers\n",
    "actor_optimizer = optim.Adam(update_actor.parameters())\n",
    "critic_optimizer = optim.Adam(update_critic.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85bfff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "### Use random weights as baseline\n",
    "########\n",
    "#untrained_actor = type(target_actor)(state_size, action_size).to(device=device)\n",
    "#untrained_actor.load_state_dict(untrained_actor.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed0a768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num batches: 1210\n",
      "At epoch 0, iter 0: train Q loss = 0.36700448393821716\n",
      "At epoch 0, iter 0: train Actor loss = 0.14679290354251862\n",
      "At epoch 0, iter 0: val Q loss = 0.3934997022151947\n",
      "At epoch 0, iter 0: val Actor loss = 1.6569722890853882\n",
      "At epoch 0, iter 403: train Q loss = 0.04742977023124695\n",
      "At epoch 0, iter 403: train Actor loss = -0.684864342212677\n",
      "At epoch 0, iter 403: val Q loss = 0.31308141350746155\n",
      "At epoch 0, iter 403: val Actor loss = 3.1866886615753174\n",
      "At epoch 0, iter 806: train Q loss = 0.04839809238910675\n",
      "At epoch 0, iter 806: train Actor loss = -1.6457628011703491\n",
      "At epoch 0, iter 806: val Q loss = 0.31144979596138\n",
      "At epoch 0, iter 806: val Actor loss = 3.203526020050049\n",
      "At epoch 0, iter 1209: train Q loss = 0.04650387167930603\n",
      "At epoch 0, iter 1209: train Actor loss = -2.14587664604187\n",
      "At epoch 0, iter 1209: val Q loss = 0.30496200919151306\n",
      "At epoch 0, iter 1209: val Actor loss = 3.2190263271331787\n",
      "At epoch 1, iter 0: train Q loss = 0.4030025601387024\n",
      "At epoch 1, iter 0: train Actor loss = -2.2479143142700195\n",
      "At epoch 1, iter 0: val Q loss = 0.30316993594169617\n",
      "At epoch 1, iter 0: val Actor loss = 3.223928451538086\n",
      "At epoch 1, iter 403: train Q loss = 0.12226574122905731\n",
      "At epoch 1, iter 403: train Actor loss = -3.13380765914917\n",
      "At epoch 1, iter 403: val Q loss = 0.3075462579727173\n",
      "At epoch 1, iter 403: val Actor loss = 3.1899642944335938\n",
      "At epoch 1, iter 806: train Q loss = 0.16310864686965942\n",
      "At epoch 1, iter 806: train Actor loss = -4.122418403625488\n",
      "At epoch 1, iter 806: val Q loss = 0.3391968309879303\n",
      "At epoch 1, iter 806: val Actor loss = 3.2485971450805664\n",
      "At epoch 1, iter 1209: train Q loss = 0.30916133522987366\n",
      "At epoch 1, iter 1209: train Actor loss = -4.663133144378662\n",
      "At epoch 1, iter 1209: val Q loss = 0.3790927827358246\n",
      "At epoch 1, iter 1209: val Actor loss = 3.414837598800659\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training loop\n",
    "\n",
    "Need to add action maximum, else exploding\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "break_var = False\n",
    "training_q_loss_list = []\n",
    "training_actor_loss_list = []\n",
    "val_q_loss_list = []\n",
    "val_actor_loss_list = []\n",
    "\n",
    "num_batches = int(len(train_data)/BATCH_SIZE)+1\n",
    "\n",
    "print(f\"Num batches: {num_batches}\")\n",
    "\n",
    "COPY_TARGETS_INDEX = int(num_batches/5)\n",
    "\n",
    "for k in range(epochs):\n",
    "    \n",
    "    update_actor.train()\n",
    "    update_critic.train()\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "\n",
    "        # update weights of target network\n",
    "        if i % COPY_TARGETS_INDEX == 0:\n",
    "            target_critic.load_state_dict(update_critic.state_dict())\n",
    "            target_actor.load_state_dict(update_actor.state_dict())\n",
    "\n",
    "\n",
    "        #play = sample_from_plays(BATCH_SIZE, train_data, seed=k)\n",
    "        # get sample from data\n",
    "        start_index = i*BATCH_SIZE\n",
    "        end_index = min(len(train_data), (i+1)*BATCH_SIZE)\n",
    "\n",
    "        play = train_data.iloc[list(range(start_index, end_index)), :]\n",
    "\n",
    "        state = torch.tensor(np.stack(play['state'].values), dtype=torch.float32).to(device=device)\n",
    "        action = torch.tensor(np.stack(play['action'].values), dtype=torch.float32).to(device=device)\n",
    "        reward = torch.tensor(play['reward'].values, dtype=torch.float32).to(device=device)\n",
    "        next_state = torch.tensor(np.stack(play['next_state'].values), dtype=torch.float32).to(device=device)\n",
    "        next_action = torch.tensor(np.stack(play['next_action'].values), dtype=torch.float32).to(device=device)\n",
    "\n",
    "        # do this because forget to replace nans in next state in dataset construction\n",
    "        next_state = torch.nan_to_num(next_state, nan=0)\n",
    "        \n",
    "        # Why does a state have nan values?\n",
    "        state = torch.nan_to_num(state, nan=0)\n",
    "\n",
    "        #update_actor, update_critic, target_actor, target_critic\n",
    "\n",
    "        #print('next state', next_state)\n",
    "        #state_nan = torch.isnan(next_state)\n",
    "        #sum_state = sum(state_nan)\n",
    "        #print('sum state', sum_state)\n",
    "        #print('state nan', state_nan)\n",
    "        #print('tens')\n",
    "        #print(next_state[10:13, :])\n",
    "        \n",
    "        # begin updating critic\n",
    "        target_action = target_actor(next_state)\n",
    "        \n",
    "        #print('target action', target_action)\n",
    "        target_q_input = torch.cat((next_state, target_action), 1)\n",
    "        target_q_output = target_critic(target_q_input)\n",
    "        target_q_values = torch.unsqueeze(reward,1) + gamma*target_q_output\n",
    "        \n",
    "        #print(f\"target q values: {target_q_values}\")\n",
    "        \n",
    "        # get q values for current state\n",
    "        current_q_values = update_critic(torch.cat((state, action), 1))\n",
    "        #print(f\"curr q values: {current_q_values}\")\n",
    "        # calculate loss\n",
    "        critic_loss = target_loss_fn(target_q_values, current_q_values)\n",
    "\n",
    "        #print('critic loss', critic_loss)\n",
    "        \n",
    "        # update critic\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "        \n",
    "        #print(f\"State: {state.shape}\")\n",
    "        out = update_actor(state)\n",
    "        #print(f\"actor model: {out}\")\n",
    "\n",
    "        # take mean across batched samples to get scalar\n",
    "        actor_input = torch.cat((state, update_actor(state)), 1)\n",
    "        #print(f\"actor input: {actor_input}\")\n",
    "        actor_loss = -update_critic(actor_input).mean()\n",
    "        \n",
    "        #print(f\"actor loss: {actor_loss}\")\n",
    "\n",
    "        # update actor\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        training_q_loss_list.append(critic_loss)\n",
    "        training_actor_loss_list.append(actor_loss)\n",
    "\n",
    "        if (i % int(num_batches/3) == 0): #and (i != 0):\n",
    "            val_q_loss, val_actor_loss = test_loop(val_data, update_actor, update_critic, F.mse_loss, device)\n",
    "            val_q_loss_list.append((k,val_q_loss))\n",
    "            val_actor_loss_list.append((k,val_actor_loss))\n",
    "            print(f\"At epoch {k}, iter {i}: train Q loss = {critic_loss}\")\n",
    "            print(f\"At epoch {k}, iter {i}: train Actor loss = {actor_loss}\")\n",
    "            print(f\"At epoch {k}, iter {i}: val Q loss = {val_q_loss}\")\n",
    "            print(f\"At epoch {k}, iter {i}: val Actor loss = {val_actor_loss}\")\n",
    "            update_actor.train()\n",
    "            update_critic.train()\n",
    "\n",
    "        #if len(loss_list) > 1:\n",
    "        #    if loss_list[-1][1] > loss_list[-2][1]:\n",
    "        #        break_var = True\n",
    "                #break\n",
    "                    \n",
    "        #if break_var:\n",
    "        #    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7493d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(update_actor.state_dict(), 'saved_models/actor_critic_actor.pt')\n",
    "#torch.save(update_critic.state_dict(), 'saved_models/actor_critic_critic.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e897381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### compare random baseline on test data\n",
    "test_q_loss, test_actor_loss = test_loop(test_data, update_actor, update_critic, F.mse_loss, device)\n",
    "\n",
    "print(f\"Trained Critic test MSE: {test_q_loss}\")\n",
    "print(f\"Trained Actor model test MSE: {test_actor_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0378b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_actions(df, row_index, update_actor, update_critic, max_action,verbose=False):\n",
    "    \n",
    "    update_actor.eval()\n",
    "    update_critic.eval()\n",
    "\n",
    "    sample_row = df.iloc[row_index,:]\n",
    "    \n",
    "    state = torch.tensor(sample_row['state'], dtype=torch.float32).to(device=device)\n",
    "    action = torch.tensor(sample_row['action'], dtype=torch.float32).to(device=device)\n",
    "    next_action = torch.tensor(sample_row['next_action'], dtype=torch.float32).to(device=device)\n",
    "    \n",
    "    actor_next_action = update_actor(torch.unsqueeze(state,0)).detach().cpu()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.scatter(next_action[0].detach().cpu(), next_action[1].detach().cpu(), c='red', s=50)\n",
    "    ax.scatter(actor_next_action[0,0], actor_next_action[0,1], c='blue', s=50)\n",
    "    ax.annotate('TRUE', (next_action[0].detach().cpu(), next_action[1].detach().cpu()))\n",
    "    ax.annotate('PREDICTED', (actor_next_action[0,0].detach().cpu(), actor_next_action[0,1].detach().cpu()))\n",
    "    plt.title(\"True vs Predicted Action\")\n",
    "    plt.xlabel(\"X pos\")\n",
    "    plt.ylabel(\"Y pos\")\n",
    "    plt.xlim(-max_action[0], max_action[0])\n",
    "    plt.ylim(-max_action[1], max_action[1])\n",
    "\n",
    "    plt.savefig(\"general_images/actor_critic_action_plot.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_action)\n",
    "\n",
    "for k in range(3,4):\n",
    "    compare_actions(test_data, k, update_actor, update_critic, max_action.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783644de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history_list, metric, filename):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # plotting\n",
    "    ax.plot(list(range(1,len(history_list)+1)), history_list)\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(f\"{metric}\")\n",
    "    plt.show()\n",
    "\n",
    "    #file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)\n",
    "    fig.savefig(filename)\n",
    "    \n",
    "q_loss_list = [0.4091, 0.3140, 0.3079, 0.3054, 0.3026, 0.3057, 0.3069, 0.3074]\n",
    "#plot_history(q_loss_list, 'MSE Loss', 'training_plots/actor_critic_q_loss.png')\n",
    "\n",
    "plot_actor_loss_list = [2.9610, 3.1350, 3.2589,3.2115,3.1616, 3.2048, 3.2249]\n",
    "plot_history(plot_actor_loss_list, 'MSE Actor Loss', 'training_plots/actor_critic_actor_loss.png')\n",
    "#print(val_actor_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aeef9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed0ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
